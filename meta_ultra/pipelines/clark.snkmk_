import pandas as pd
import sys
import os.path
import os

paired = config['PAIRED_END'].lower() in ['y', 'true', 'yes', 't']
if not paired:
	sys.stderr.write('Running in Single Ended Mode\n')
else:
	sys.stderr.write('Running in Paired Ended Mode\n')
	
if not os.path.isdir(config['OUTPUT_DIR']):
	os.mkdir(config['OUTPUT_DIR'])

onsuccess:
	sys.stderr.write('MUP complete\n')

onerror:
	sys.stderr.write('An error occurred.')

localrules: clark_make_sample_and_result_files, minhash_sketch_single, mash_dists_single

include: 'modules/shortbred.snkmk'


def getReads(wildcards):
	reads = []
	reads.append(config['SAMPLE_DIR'] + wildcards.sample + config['READ_1_EXT'])
	if paired:
		reads.append(config['SAMPLE_DIR'] + wildcards.sample + config['READ_2_EXT'])
	return reads

def getCleanReads(wildcards):
	reads = []
	reads.append(config['OUTPUT_DIR']+ wildcards.sample+'_nothost.fastq.gz')
	return reads

def all_input(wildcards):
	tools = config['TOOLS_TO_RUN']
	inp = []
        inp += shortbred_all()

	if 'METAPHLAN2' in tools:
		inp += expand(config['OUTPUT_DIR']+'{sample}'+config['METAPHLAN2']['EXT'], sample=config['SAMPLES'].keys())
	if 'PANPHLAN' in tools:
		inp += expand('{sample}.{species}.panphlan.flag', sample=config['SAMPLES'], species=config['PANPHLAN']['DBS'])
#		inp += expand(config['OUTPUT_DIR']+'{sample}.{species}'+config['PANPHLAN']['EXT'], sample=config['SAMPLES'].keys(), species=config['PANPHLAN']['DBS'])
	if 'MICROBE_CENSUS' in tools:
		inp += expand(config['OUTPUT_DIR']+'{sample}'+config['MICROBE_CENSUS']['EXT'], sample=config['SAMPLES'].keys())
	if 'KRAKEN' in tools:
		inp += expand(config['OUTPUT_DIR']+'{sample}'+config['KRAKEN']['MPA_EXT'], sample=config['SAMPLES'].keys())
	if 'CLARK' in tools:
		pass
		#inp += expand(config['OUTPUT_DIR']+'{sample}'+config['CLARK']['EXT'], sample=config['SAMPLES'].keys())
	if 'KNEADDATA' in tools:
		inp += expand(config['OUTPUT_DIR'] + '{sample}_host.fastq.gz', sample=config['SAMPLES'])
		inp += expand(config['OUTPUT_DIR'] + '{sample}_nothost.fastq.gz', sample=config['SAMPLES'])
		pass # currently knead data is only called by other tools
	if 'HUMANN2' in tools:
		pass
		inp += expand( config['OUTPUT_DIR'] + '{sample}_humann2/{sample}_genefamilies.tsv', sample=config['SAMPLES'].keys())
		inp += expand( config['OUTPUT_DIR'] + '{sample}_humann2/{sample}_pathabundance.tsv', sample=config['SAMPLES'].keys())
		inp += expand( config['OUTPUT_DIR'] + '{sample}_humann2/{sample}_pathcoverage.tsv', sample=config['SAMPLES'].keys())
	if 'COUNT_CLASSIFIED' in tools:
		inp += expand( config['OUTPUT_DIR']+'{sample}.reads_classified.json', sample=config['SAMPLES'].keys())
	if 'MASH' in tools:
		inp += expand( config['OUTPUT_DIR']+'{sample}.hmp_sites.json', sample=config['SAMPLES'].keys())
	return inp

rule all:
	input:
		all_input

		
# MetaPhlAn2
rule metaphlan2_single_sample:
	input:
		reads = getReads
	output:
		main=config['OUTPUT_DIR']+'{sample}' + config['METAPHLAN2']['EXT'],
		bt2=temp('{sample}' + config['METAPHLAN2']['EXT'] + '.bt2')
	threads: int(config['METAPHLAN2']['THREADS'])
	version: config['METAPHLAN2']['VERSION']
	params:
		metaphlan2=config['METAPHLAN2']['EXC'],
		job_name=config['JOB_NAME_PREFIX'] + 'metaphlan2_single_sample',
	resources:
		time=int(config['METAPHLAN2']['TIME']),
		n_gb_ram=int(config['METAPHLAN2']['RAM'])

	run:
		if paired:
			cmd = '{params.metaphlan2} --input_type fastq {input.reads[0]},{input.reads[1]} --nproc {threads} --bowtie2out {output.bt2} > {output.main}'
		else:
			cmd = '{params.metaphlan2} --input_type fastq {input.reads[0]} --nproc {threads} --bowtie2out {output.bt2} > {output.main}'
		shell(cmd)

# Kraken
rule kraken_raw_single_sample:
	input:
		reads = getCleanReads
	output:
		main = temp(config['OUTPUT_DIR']+'{sample}' + config['KRAKEN']['RAW_EXT'])
	threads: int( config['KRAKEN']['THREADS'])
	version: config['KRAKEN']['VERSION']
	params:
		kraken = config['KRAKEN']['EXC'],
		db = config['KRAKEN']['DB'],
		job_name=config['JOB_NAME_PREFIX'] + 'kraken_raw_single_sample'
	resources:
		time=int(config['KRAKEN']['TIME']),
		n_gb_ram=int(config['KRAKEN']['RAM'])
	run:
		if paired:
			cmd = '{params.kraken} --gzip-compressed --fastq-input --threads {threads} '
			cmd += '--paired --preload --db {params.db} {input.reads[0]} {input.reads[1]} > {output.main}'
		else:
			cmd = '{params.kraken} --gzip-compressed --fastq-input --threads {threads} '
			cmd += '--preload --db {params.db} {input.reads[0]} > {output.main}'
		shell(cmd)


rule kraken_mpa_single:
	input:
		raw = config['OUTPUT_DIR']+'{sample}' + config['KRAKEN']['RAW_EXT']
	output:
		main = config['OUTPUT_DIR']+'{sample}' + config['KRAKEN']['MPA_EXT']
	threads: 1
	version: config['KRAKEN']['VERSION']
	params:
		kraken_mpa = config['KRAKEN']['MPA_EXC'],
		db = config['KRAKEN']['DB'],
		job_name=config['JOB_NAME_PREFIX'] + 'kraken_mpa_single'
	resources:
		time=int(config['KRAKEN']['MPA_TIME']),
		n_gb_ram=int(config['KRAKEN']['MPA_RAM'])
	shell:
		'{params.kraken_mpa} {input.raw} --db {params.db} > {output.main}'




		
# PanPhlAn
rule panphlan_make_bam:
	input:
		reads = getReads
	output:
		main = temp(config['OUTPUT_DIR']+'{sample}.{species}.bt2.bam')
	threads: int( config['PANPHLAN']['THREADS'])
	params:
		job_name=config['JOB_NAME_PREFIX'] + 'panphlan_make_bam',
		bt2 = config['BOWTIE2'],
		samtools = config['SAMTOOLS'],
		bt2_index = config['PANPHLAN']['DB_DIR'] + '{species}'
	resources:
		time=int(config['PANPHLAN']['BT2_TIME']),
		n_gb_ram=int(config['PANPHLAN']['BT2_RAM'])
	run:
		if paired:
			cmd = '{params.bt2} --threads {threads} -x {params.bt2_index} -1 {input.reads[0]} -2 {input.reads[1]} | '

		else:
			cmd = '{params.bt2} --threads {threads} -x {params.bt2_index} -U {input.reads[0]} | '
			cmd += '{params.samtools} view -F 4 -b > {output.main} '
		shell(cmd)

rule panphlan_sort_bam:
	input:
		bam = config['OUTPUT_DIR']+'{sample}.{species}.bt2.bam'
	output:
		main = temp( config['OUTPUT_DIR']+'{sample}.{species}.sorted.bt2.bam')
	threads: 1
	params:
		job_name=config['JOB_NAME_PREFIX'] + 'panphlan_sort_bam',
		samtools = config['SAMTOOLS']
	resources:
		time=1,
		n_gb_ram=4
	shell:
		'{params.samtools} sort {input.bam} > {output.main}'


rule panphlan_index_bam:
	input:
		bam = config['OUTPUT_DIR']+'{sample}.{species}.sorted.bt2.bam'
	output:
		main = temp( config['OUTPUT_DIR']+'{sample}.{species}.sorted.bt2.bam.bai')
	threads: 1
	params:
		job_name=config['JOB_NAME_PREFIX'] + 'panphlan_index_bam',
		samtools = config['SAMTOOLS']
	resources:
		time=1,
		n_gb_ram=4
	shell:
		'{params.samtools} index {input.bam}'

		
rule panphlan_map_single:
	input:
		bam = config['OUTPUT_DIR']+'{sample}.{species}.sorted.bt2.bam',
		bai = config['OUTPUT_DIR']+'{sample}.{species}.sorted.bt2.bam.bai'
	output:
		flag='{sample}.{species}.panphlan.flag'
	threads: int( config['PANPHLAN']['THREADS'])
	params:
		job_name=config['JOB_NAME_PREFIX'] + 'panphlan_map_single',
		p_map = config['PANPHLAN']['EXC'],
		clade = '{species}',
		bt2_indices = config['PANPHLAN']['DB_DIR'],
		main_out = config['OUTPUT_DIR']+'{sample}.{species}' + config['PANPHLAN']['EXT'] 
	resources:
		time=int(config['PANPHLAN']['TIME']),
		n_gb_ram=int(config['PANPHLAN']['RAM'])
	shell:
		'{params.p_map} -p {threads} -c {params.clade} -i {input.bam}  -o {params.main_out} --i_bowtie2_indexes {params.bt2_indices} '
		' && touch {output.flag}'

# MicrobeCensus
rule microbe_census_single:
	input:
		reads = getReads
	output:
		main = config['OUTPUT_DIR']+'{sample}' + config['MICROBE_CENSUS']['EXT']
	threads: int( config['MICROBE_CENSUS']['THREADS'])
	version: config['MICROBE_CENSUS']['VERSION']
	params:
		mic_census = config['MICROBE_CENSUS']['EXC'],
		job_name=config['JOB_NAME_PREFIX'] + 'mic_census_single'
	resources:
		time=int(config['MICROBE_CENSUS']['TIME']),
		n_gb_ram=int(config['MICROBE_CENSUS']['RAM'])
	run:
		if paired:
			cmd='{params.mic_census} -t {threads} {input.reads[0]},{input.reads[1]} {output.main}'
		else:
			cmd='{params.mic_census} -t {threads} {input.reads[0]} {output.main}'
		shell(cmd)


# MetaBGC	
	
# CLARK
# http://clark.cs.ucr.edu/Overview/
'''
rule clark_make_sample_and_result_files:
	input:
		f1s = expand(config['SAMPLE_DIR'] + '{sample}'+ config['READ_1_EXT'], sample=config['SAMPLES'].keys()),
		f2s = expand(config['SAMPLE_DIR'] + '{sample}'+ config['READ_2_EXT'], sample=config['SAMPLES'].keys())
	output:
		s1=temp('clark_samples_1.txt'),
		s2=temp('clark_samples_2.txt'),
		r=temp('clark_results.txt')
	threads: int( config['CLARK']['THREADS'])
	version: config['CLARK']['VERSION']
	params:
		samples = config['SAMPLES'],
		job_name=config['JOB_NAME_PREFIX'] + 'clark_makefiles_single'
	run:
		with open(output.s1,'w') as s1, open(output.s2,'w') as s2, open(output.r, 'w') as r:
			for sampleName, fileDict in params.samples.items():
				s1.write(config['SAMPLE_DIR'] + fileDict['1'] + '\n')
				s2.write(config['SAMPLE_DIR'] + fileDict['2'] + '\n')
				r.write(sampleName + config['CLARK']['EXT'] + '\n')
	
rule clark_all:
	input:
		s1='clark_samples_1.txt',
		s2='clark_samples_2.txt',
		r='clark_results.txt'
	output:
		expand(config['OUTPUT_DIR']+ '{sample}' + config['CLARK']['EXT'], sample=config['SAMPLES'].keys())
	threads: int( config['CLARK']['THREADS'])
	version: config['CLARK']['VERSION']
	params:
		clark_classify = config['CLARK']['EXC'],
		job_name=config['JOB_NAME_PREFIX'] + 'clark_all'
	resources:
		time=int(config['CLARK']['TIME']),
		n_gb_ram=int(config['CLARK']['RAM'])
	shell:
		'{params.clark_classify} -P {input.s1} {input.s2} -R {input.r}'
'''

# KneadData (QC)
rule kneadata_single:
	input:
		reads = getReads
	output:
		clean=temp(config['OUTPUT_DIR']+'{sample}_{knead_db}_kneaddata/{sample}_kneaddata.fastq'),
		contam=temp(config['OUTPUT_DIR']+'{sample}_{knead_db}_kneaddata/{sample}_kneaddata_{knead_db}_bowtie2_contam.fastq'),
		flag=config['OUTPUT_DIR'] + '{sample}.{knead_db}.kneaddata.flag'
		
	threads: int( config['KNEADDATA']['THREADS'])
	params:
		db = config['KNEADDATA']['DB'] ,
		odir = temp(config['OUTPUT_DIR']+'{sample}_{knead_db}_kneaddata/'),
		sprefix='{sample}_kneaddata',
		job_name=config['JOB_NAME_PREFIX'] + 'kneaddata_single',
		exc= config['KNEADDATA']['EXC']
	resources:
		time=int(config['KNEADDATA']['TIME']),
		n_gb_ram=int(config['KNEADDATA']['RAM'])
	run:
		if paired: 
			cmd = '{params.exc} -i {input.reads[0]} -i {input.reads[1]} -db {params.db} --bypass-trim '
			cmd += '-o {params.odir} -t {threads} --output-prefix {params.sprefix} ; '
			cmd += 'touch {output.flag}' # kneaddata doesn't work yet but it fails AFTER the files we need are produced
		else: 
			cmd = '{params.exc} -i {input.reads[0]} -db {params.db} --bypass-trim '
			cmd += '-o {params.odir} -t {threads} --output-prefix {params.sprefix} ; '
			cmd += 'touch {output.flag}' # kneaddata doesn't work yet but it fails AFTER the files we need are produced
		shell(cmd)

# Filter Host Reads
rule filter_host_reads_single:
	input:
		reads = getReads
	output:
		host=config['OUTPUT_DIR']+'{sample}_host.fastq.gz',
		nothost=config['OUTPUT_DIR']+'{sample}_nothost.fastq.gz',
	threads: int( config['KNEADDATA']['THREADS'])
	params:
		db = config['KNEADDATA']['DB'] ,
		job_name=config['JOB_NAME_PREFIX'] + 'filter_host_single',
		bt2= config['BOWTIE2']
	resources:
		time=int(config['KNEADDATA']['TIME']),
		n_gb_ram=int(config['KNEADDATA']['RAM'])
	run:
		if paired:
			cmd = '{params.bt2} --threads {threads} -x {params.db} -1 {input.reads[0]} -2 {input.reads[1]} | '
		else:
			cmd = '{params.bt2} --threads {threads} -x {params.db} -U {input.reads[0]} '
			cmd += '--un-gz {output.nothost} --al-gz {output.host} > /dev/null' 

		shell(cmd)

# Count classified, unclassified reads
rule count_classified_reads_single:
	input:
		host=config['OUTPUT_DIR'] + '{sample}_host.fastq.gz',
		nothost=config['OUTPUT_DIR'] + '{sample}_nothost.fastq.gz',
		krakenmpa=config['OUTPUT_DIR']+'{sample}' + config['KRAKEN']['MPA_EXT']
	output:
		main=config['OUTPUT_DIR']+'{sample}.reads_classified.json'
	params:
		script=config['COUNT_CLASSIFIED']['EXC'],
		job_name=config['JOB_NAME_PREFIX']+'count_classified_reads_single'
	resources:
		time=1,
		n_gb_ram=2
	threads: 1
	shell:
		'{params.script} {input.host} {input.nothost} {input.krakenmpa} > {output.main}'


		
# HUMAnN2
def microbeCensusGetReads(wildcards):
	reads = config['SAMPLE_DIR'] + wildcards.sample + config['READ_1_EXT']
	if paired:
		reads += ','
		reads += config['SAMPLE_DIR'] + wildcards.sample + config['READ_2_EXT']
	return reads

rule humann2_make_blastm8:
	input:
		k1 = getCleanReads,
		dmnd_db = config['HUMANN2']['DB'] 
	output:
		main = temp(config['OUTPUT_DIR']+'{sample}.m8')
	threads: int( config['HUMANN2']['DMND_THREADS'])
	params:
		dmnd = config['DIAMOND'],
		job_name=config['JOB_NAME_PREFIX'] + 'humann2_make_blastm8',
		bsize=(int( config['HUMANN2']['DMND_THREADS']) *int(config['HUMANN2']['DMND_RAM']))/6 - 2 
	resources:
		time=int(config['HUMANN2']['DMND_TIME']),
		n_gb_ram=int(config['HUMANN2']['DMND_RAM'])
	shell:
		'{params.dmnd} blastx --threads {threads} -d {input.dmnd_db} -q {input.k1} -o {output.main} --block-size {params.bsize}'


rule humann2_single:
	input:
		m8 = config['OUTPUT_DIR']+'{sample}.m8'
	output:
		config['OUTPUT_DIR'] + '{sample}_humann2/{sample}_genefamilies.tsv',
		config['OUTPUT_DIR'] + '{sample}_humann2/{sample}_pathabundance.tsv',
		config['OUTPUT_DIR'] + '{sample}_humann2/{sample}_pathcoverage.tsv'
	threads: int( config['HUMANN2']['THREADS'])
	params:
		exc= config['HUMANN2']['EXC'],
		odir = config['OUTPUT_DIR'] + '{sample}_humann2',
		job_name=config['JOB_NAME_PREFIX'] + 'humann2_single'
	resources:
		time=int(config['HUMANN2']['TIME']),
		n_gb_ram=int(config['HUMANN2']['RAM'])
	shell:
		'{params.exc} --input {input.m8} --output {params.odir}'


rule minhash_sketch_single:
	input:
		clean = getCleanReads,
	output:
		msh = config['OUTPUT_DIR'] + '{sample}.msh',
	threads: 1
	params:
		exc=config['MASH']['EXC'],
		job_name=config['JOB_NAME_PREFIX'] + '{sample}_mash_sketch_single',
	resources:
		time=1,
		n_gb_ram=2
	run:
		if paired:
			pass
		else:
			cmd = '{params.exc} sketch -s 10000 -o {output.msh} {input.clean[0]}'
		shell(cmd)


rule mash_dists_single:
	input:
		msh= config['OUTPUT_DIR']+'{sample}.msh',
		ref= config['MASH']['REF']
	output:
		main=config['OUTPUT_DIR'] + '{sample}.hmp_sites.json'
	resources:
		time=1,
		n_gb_ram=2
	threads: 1
	params:
		script=config['MASH']['DIST_SCRIPT'],
		job_name=config['JOB_NAME_PREFIX'] + '{sample}_mash_dists_single'
	shell:
		'{params.script} {input.ref} {input.msh} > {output.main}'



